{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#import torch.utils.data.dataloader as dataloader\n",
    "#from torch.utils.data import DataLoader\n",
    "#import torch.optim as optim\n",
    "#import pandas as pd\n",
    "#import os\n",
    "\n",
    "#from torch.utils.data import Dataset\n",
    "\n",
    "#from torch.utils.data import TensorDataset\n",
    "#from torchvision import transforms\n",
    "#from torchvision.datasets import MNIST\n",
    "#import torchvision\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import time\n",
    "#from IPython.display import clear_output\n",
    "#from skimage import io\n",
    "\n",
    "#https://www.youtube.com/watch?v=FnDj8Jdu-X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdfc0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48566143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9de5ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ucr(filename):\n",
    "    data_import = np.loadtxt(filename, delimiter = ',')\n",
    "    # forman: Each row is a sample. The first item in each row is the label, the rest is the time-series data\n",
    "    label = data_import[:, 0]\n",
    "    data = data_import[:, 1:]\n",
    "    return data, label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2cabca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load time-series data\n",
    "\n",
    "training_dir = 'C:/Users/mango/OneDrive/Documents/Uni/FYP/PuTTY logs/testing/files_train_all.csv'\n",
    "testing_dir = 'C:/Users/mango/OneDrive/Documents/Uni/FYP/PuTTY logs/testing/files_test_all.csv'\n",
    "\n",
    "x_train, y_train = read_ucr(training_dir)\n",
    "x_test, y_test = read_ucr(testing_dir)\n",
    "\n",
    "#perform the scaling for the Relu activation function\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(np.transpose(x_train))\n",
    "x_train = np.transpose(scaler.transform(np.transpose(x_train)))\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(np.transpose(x_test))\n",
    "x_test = np.transpose(scaler.transform(np.transpose(x_test)))\n",
    "\n",
    "##get required stats about the data\n",
    "#length of the time series\n",
    "ts_len = x_train.shape[1]\n",
    "#the number of train and test\n",
    "num_train = x_train.shape[0]\n",
    "num_test = x_test.shape[0]\n",
    "#number of classes\n",
    "num_classes = len(np.unique(y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e872154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#labels\n",
    "\n",
    "#adjust the labels so that they are 0 - whatever\n",
    "\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(num_classes-1)\n",
    "y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(num_classes-1)\n",
    "\n",
    "#one-hot encode the labels\n",
    "\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "123a806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this\n",
    "#def plot_grid_timeseries():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "727a5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#batch size must be changed!!!\n",
    "#batch_size = 25\n",
    "batch_size = num_train\n",
    "\n",
    "num_channels = 1\n",
    "n_epochs = 1000\n",
    "\n",
    "##original\n",
    "##convolutional layers\n",
    "##layer 1\n",
    "##filter_size1 = 5\n",
    "#filter_size1 = 10\n",
    "#num_filters1 = 25\n",
    "\n",
    "\n",
    "##layer 2\n",
    "#filter_size2 = 5\n",
    "#num_filters2 = 50\n",
    "\n",
    "\n",
    "#convolutional layers\n",
    "#layer 1\n",
    "#filter_size1 = 5\n",
    "filter_size1 = 30\n",
    "num_filters1 = 10\n",
    "\n",
    "\n",
    "#layer 2\n",
    "filter_size2 = 10\n",
    "num_filters2 = 30\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "393c5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "#construct the convolutional layer\n",
    "\n",
    "def conv_layer(value, num_input_channels, filter_size, num_filters, use_pooling=True):\n",
    "    \n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    \n",
    "    #shape of the filter-weights for the convolution\n",
    "    #this format is determined by the tesnorflow api\n",
    "    shape = [filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    #create new weights (or filters) with the given shape\n",
    "    weights = new_weights(shape=shape)\n",
    "    \n",
    "    #create new biases, one for each filter\n",
    "    biases = new_biases(length=num_filters)\n",
    "    \n",
    "    #create the tensorflow operation for the convolution\n",
    "    #a 1d convolution is performaed on the time-series with stride 1\n",
    "    #the padding is set to 'same' so that the ourput is the same size as the input\n",
    "    layer = tf.compat.v1.nn.conv1d(value=value, filters=weights, stride=1, padding='SAME')\n",
    "    \n",
    "    #add the biases to the results of the convolution\n",
    "    layer += biases\n",
    "    \n",
    "    #activation function\n",
    "    #re ReLu activation function is used\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    #pooling is used to downsamlpe the time-series\n",
    "    if use_pooling:\n",
    "        #max pooling is used where a pool size of 2 is used\n",
    "        layer = tf.compat.v1.layers.max_pooling1d(inputs=layer, pool_size=2, strides=2, padding='SAME')\n",
    "        \n",
    "    #return the resulting layer and the weights\n",
    "    return layer, weights\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1dfc6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct the fully connected layer\n",
    "\n",
    "def new_fc_layer(input, num_inputs, num_outputs, use_relu=True):\n",
    "    \n",
    "    #create new weights and biases\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "    \n",
    "    #calculate the layer as the matrix multiplication of the input and weights, and the add the bias-values\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    \n",
    "    #use the specified activation function\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    #return the resulting layer\n",
    "    return layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b882ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the output of the convolutional layer\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    \n",
    "    #get the shape of the input layer\n",
    "    layer_shape = layer.get_shape()\n",
    "    \n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    #reshape the layer\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    \n",
    "    #return both the flattened layer and the number of features\n",
    "    return layer_flat, num_features\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56486906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise\n",
    "\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.compat.v1.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bda5af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct the convolutional neural network\n",
    "\n",
    "#set up the placeholder tensors\n",
    "#input data\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#replace 'tf' with 'tf.compat.v1'\n",
    "\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, ts_len])\n",
    "x_train_shape = tf.reshape(x, [-1, ts_len, num_channels])\n",
    "\n",
    "#labels\n",
    "y = tf.compat.v1.placeholder(tf.float32, [None, num_classes])\n",
    "y_true = tf.compat.v1.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_class = tf.argmax(y_true, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "129c6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolutional layers\n",
    "\n",
    "#layer 1 - convolutional neural layer\n",
    "layer_conv1, weights_conv1 = \\\n",
    "    conv_layer(value=x_train_shape,\n",
    "               num_input_channels=1,\n",
    "               filter_size=filter_size1,\n",
    "               num_filters=num_filters1,\n",
    "               use_pooling=True)\n",
    "\n",
    "\n",
    "#layer 1 - convolutional neural layer\n",
    "layer_conv2, weights_conv2 = \\\n",
    "    conv_layer(value=layer_conv1,\n",
    "               num_input_channels=num_filters1,\n",
    "               filter_size=filter_size2,\n",
    "               num_filters=num_filters2,\n",
    "               use_pooling=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a9dcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other layers\n",
    "\n",
    "#layer 3 - flatten the ourput from layer 2, the convolutional layer\n",
    "layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "\n",
    "#layer 4 - fully connected layer\n",
    "layer_fc1 = new_fc_layer(input=layer_flat, num_inputs=num_features, num_outputs=num_classes, use_relu=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5677a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax to perform classification\n",
    "\n",
    "#layer 5 - softmax layer\n",
    "y_pred = tf.nn.softmax(layer_fc1)\n",
    "\n",
    "#get the predicted class from the softmax layer\n",
    "y_pred_class = tf.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3f6f7916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cost function\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc1, labels=y_true)\n",
    "\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "##optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimimze(cost)\n",
    "\n",
    "#optimizer = tf.compat.v1.keras.optimizers.Adam(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(y_pred_class, y_true_class)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c612bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct the tensorflow graph\n",
    "\n",
    "#get the tensorflow session\n",
    "session = tf.compat.v1.Session()\n",
    "\n",
    "#initialise the session variables\n",
    "session.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "852f0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to construct the batches for each epoch\n",
    "\n",
    "\n",
    "#split the training data into batches\n",
    "def batch_construction(batch_size, x_train, y_train_one_hot, shuffle=True):\n",
    "    \n",
    "    num_train, ts_len = x_train.shape\n",
    "    \n",
    "    #shuffle the data if required\n",
    "    if shuffle:\n",
    "        \n",
    "        \n",
    "#        idx_shuffle = np.rng.choice(num_train, num_train)\n",
    "        idx_shuffle = np.random.choice(num_train, num_train)\n",
    "#        x_train = np.rng.choice[idx_shuffle,:]\n",
    "        x_train = x_train[idx_shuffle,:]\n",
    "        \n",
    "    \n",
    "        y_train_one_hot = y_train_one_hot[idx_shuffle]\n",
    "    \n",
    "    \n",
    "    #split the batches in a list, one for the data, one for the lbels\n",
    "    minibatch = []\n",
    "    minibatch_labels = []\n",
    "    i=0\n",
    "    while i < num_train:\n",
    "        #extract the batch data from the enture training set\n",
    "        if i + batch_size <= num_train:\n",
    "            minibatch.append(x_train[i:i+batch_size,:])\n",
    "            minibatch_labels.append(y_train_one_hot[i:i+batch_size,:])\n",
    "        else:\n",
    "            #construct a batch with the final elements and a random selection of the previous ones\n",
    "            num_remain = num_train - i\n",
    "            final_batch = np.zeros([batch_size, num_classes])\n",
    "            final_batch_labels = np.zeros([batch_size, num_classes])\n",
    "            final_batch[:num_remain,:] = x_train[i:num_train,:]                              #error on this line\n",
    "            #ValueError: could not broadcast input array from shape (10,76) into shape (10,2)\n",
    "            \n",
    "            final_batch_labels[:num_remain,:] = y_train_one_hot[i:num_train,:]\n",
    "            \n",
    "            #select the remaining part of the batch randomly\n",
    "            num_required = batch_size - num_remain\n",
    "            \n",
    "#            idx = np.rng.choice(batch_size, num_required)\n",
    "            idx = np.random.choice(batch_size, num_required)\n",
    "            \n",
    "            final_batch[num_remain:,:] = x_train[idx,:]\n",
    "            final_batch_labels[num_remain:,:] = y_train_one_hot[idx,:]\n",
    "            minibatch.append(final_batch)\n",
    "            minibatch_labels.append(final_batch_labels)\n",
    "        #increment i\n",
    "        i += batch_size\n",
    "    \n",
    "    return zip(minibatch, minibatch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d84a8361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 Training Accuracy: 49.00\n",
      "Epoch: 101 Training Accuracy: 69.00\n",
      "Epoch: 201 Training Accuracy: 67.00\n",
      "Epoch: 301 Training Accuracy: 74.00\n",
      "Epoch: 401 Training Accuracy: 75.00\n",
      "Epoch: 501 Training Accuracy: 76.00\n",
      "Epoch: 601 Training Accuracy: 77.00\n",
      "Epoch: 701 Training Accuracy: 79.00\n",
      "Epoch: 801 Training Accuracy: 78.00\n",
      "Epoch: 901 Training Accuracy: 82.00\n",
      "Epoch: 1000 Final Training Accuracy: 82.00\n"
     ]
    }
   ],
   "source": [
    "#evaluate the epoch\n",
    "i1 = 0;\n",
    "acc_mat = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    #put each batch through the convolutional neural network\n",
    "    \n",
    "    #construct the minibatches for this epoch\n",
    "    \n",
    "    ###ARE THESE BATCH SIZES CAUSING THE ERROR??\n",
    "    \n",
    "    #batch_size = 20\n",
    "    batch_size = num_train\n",
    "    \n",
    "    minibatch = batch_construction(batch_size, x_train, y_train_one_hot)\n",
    "    \n",
    "    for c_batch in minibatch:\n",
    "        \n",
    "        #feed in the batch and updates the weights using backpropagation\n",
    "        x_batch = c_batch[0]\n",
    "        y_true_batch = c_batch[1]\n",
    "        \n",
    "        #put the batch into a dict with the proper names for placeholder variables in the tensorflow graph\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "        \n",
    "        #run the optimiser using this batch of trainingg data\n",
    "        #tensorflow assigns the variables in feed_dict_train to the placeholder variables and then reuns the optimiser\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "    \n",
    "    #use all the training data to construct the accuract after this epoch is complete\n",
    "    feed_dict_train = {x: x_train, y_true: y_train_one_hot}\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        #calculate the accuracy of the training-set\n",
    "        acc = session.run(accuracy, feed_dict=feed_dict_train)*100\n",
    "        acc_mat[i1] = acc\n",
    "        i1 += 1\n",
    "        print('Epoch: %03d Training Accuracy: %1.2f' %(epoch+1, acc))\n",
    "print('Epoch: %03d Final Training Accuracy: %1.2f' %(epoch+1, acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5870c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n",
      "Correct: 63.00\n",
      "Total: 99.00\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1\n",
      " 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1]\n",
      "[49.000000953674316, 68.99999976158142, 67.00000166893005, 74.00000095367432, 75.0, 75.99999904632568, 76.99999809265137, 79.00000214576721, 77.99999713897705, 81.99999928474426, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mango\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {x: x_test.reshape(-1, ts_len),\n",
    "            y_true: y_test}\n",
    "\n",
    "#calculate the predicted class using tensorflow\n",
    "class_pred = session.run(y_pred_class, feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "#correct_sum = (np.array(y_test == class_pred)).sum()\n",
    "#print(class_pred)\n",
    "\n",
    "#create a boolean array whether each image is correctly classified\n",
    "#m, n = size(y_test)\n",
    "#print(m)\n",
    "#print(n)\n",
    "\n",
    "#for i in range(1, len(y_test)):\n",
    "        \n",
    "#    if (y_test[i-1] == class_pred[i-1]):\n",
    "#        correct_counter += 1\n",
    "\n",
    "correct_sum = np.array(y_test == class_pred).sum()\n",
    "#print(correct_vals)\n",
    "\n",
    "#print('total correct: %1.2f' %(correct_number))\n",
    "\n",
    "correct_sum = 0;\n",
    "total_sum = 0;\n",
    "\n",
    "for i in range(1, len(y_test)):\n",
    "    total_sum += 1\n",
    "    if (y_test[i, 1]-class_pred[i]) == 0:\n",
    "        correct_sum += 1\n",
    "    #print(y_test[i, 0]-class_pred[i])\n",
    "    \n",
    "##print(correct_sum/total_sum)\n",
    "\n",
    "print(correct_sum/len(y_test))\n",
    "    \n",
    "print('Correct: %1.2f' %(correct_sum))\n",
    "print('Total: %1.2f' %(total_sum))\n",
    "    \n",
    "#correct = (y_test == class_pred)\n",
    "\n",
    "\n",
    "##acc = (float(correct_sum)/num_test)*100\n",
    "\n",
    "##print('Statistics on the testing time-series data')\n",
    "##print('Accuracy: %1.2f%%' %(acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print()\n",
    "\n",
    "\n",
    "\n",
    "print(y_test)\n",
    "print(class_pred)\n",
    "\n",
    "\n",
    "print(acc_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba060b97",
   "metadata": {},
   "source": [
    "###### calculate the number of correctly classified images\n",
    "#when summing a boolean array, false means 0 and true means 1\n",
    "correct_sum = correct.sum()\n",
    "#correct_sum = np.sum(correct)\n",
    "\n",
    "#classification accuracy is the number of correctly classified images\n",
    "#divided by the total number of images in the test set\n",
    "acc = (float(correct_sum)/num_test)*100\n",
    "\n",
    "print('Statistics on the testing time-series data')\n",
    "print('Accuracy: %1.2f%%' %(acc))\n",
    "\n",
    "\n",
    "#get the confusion matrix using sklearn\n",
    "cm = confusion_matrix(y_true=y_test,\n",
    "                     y_pred=class_pred)\n",
    "\n",
    "#print the confusion matric as text\n",
    "print('Accuracy on class 0: Correct: %i \\ %i' %(cm[0, 0],np.sum(cm[0,:])))\n",
    "print('Accuracy on class 1: Correct: %i \\ %i' %(cm[1, 1],np.sum(cm[1,:])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f97f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae413c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794ac57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
